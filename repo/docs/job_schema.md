Ja â€“ ich kann dir helfen, das **einheitlich** zu machen. Und du hast den Kern sauber beschrieben: aktuell gibtâ€™s **mehrere â€œJob-Formenâ€** (DB-Payload, Gate-Input, WebRelay-File, Worker-Interpretation), und genau da entstehen eure Blocker wie G2/`kind`-Mismatch. 

### Muss da immer ein Wrapper dazwischen?

**Praktisch ja â€“ aber nur ein einziger, kanonischer Wrapper.**
Nicht â€œWrapper Ã¼ber Wrapperâ€, sondern:

> **1 kanonisches Job-Envelope-Schema** (DecisionTrace-Style)
>
> * **Adapter** (API â†’ Envelope, DB â†’ Envelope, File â†’ Envelope, Worker â†’ Envelope)

Damit bleibt alles prÃ¼fbar, erklÃ¤rbar und Gate-kompatibel.

---

## Ziel: Ein kanonisches Schema (DecisionTrace-Style)

### **JobEnvelope v1 (KANONISCH)**

Das hier ist der Standard, auf den sich *alle* Pfade einigen:

```json
{
  "schema_version": "job_envelope_v1",
  "job_id": "uuid",
  "mission_id": "uuid|null",
  "task_id": "uuid|null",

  "intent": "agent_plan|code_analysis|file_read|file_write|walk_tree|...",
  "action": {
    "kind": "agent_plan|FILE_READ|FILE_WRITE|PATCH_APPLY|RUN_TEST|...",
    "params": {},
    "capabilities": ["FILE_READ", "FILE_WRITE"],
    "requires": {
      "source_zone": "api|internal|mesh|offgrid",
      "paths": [],
      "network": false,
      "llm": false
    }
  },

  "provenance": {
    "source_zone": "api|internal|mesh|offgrid",
    "created_at": "iso8601",
    "created_by": {"node_id": "uuid", "identity": "string"},
    "build_id": "string"
  },

  "policy_context": {
    "priority": "low|normal|high",
    "risk": "low|med|high",
    "gates_profile": "default|strict|dev"
  },

  "refs": {
    "trace_id": "uuid|null",
    "chain_id": "string|null"
  }
}
```

**Warum das dein aktuelles Problem lÃ¶st:**
Im Flow-Dokument ist der Gate-Input faktisch `job.kind="agent_plan"` â€“ und genau daran hÃ¤ngt G2. 
Mit dem Envelope ist klar getrennt:

* `intent` = semantische Absicht (z.B. â€agent_planâ€œ)
* `action.kind` = ausfÃ¼hrbarer Capability-Kind (z.B. `FILE_READ`, `RUN_TEST`, â€¦ oder bewusst `agent_plan`, wenn es wirklich eine Capability ist)

So kannst du G2 auf **action.kind** prÃ¼fen, wÃ¤hrend intent frei bleibt.

---

### **ResultEnvelope v1 (KANONISCH)**

FÃ¼r Worker-Result + DB-Update + Trace:

```json
{
  "schema_version": "result_envelope_v1",
  "job_id": "uuid",
  "ok": true,
  "status": "completed|failed|blocked",
  "error": {"code": "string", "message": "string", "details": {}},

  "result": {
    "summary": "string",
    "data": {}
  },

  "evidence": {
    "artifacts": [],
    "logs": [],
    "metrics": {}
  },

  "decision": {
    "trace_id": "uuid|null",
    "why_ref": "string|null"
  },

  "timing": {
    "started_at": "iso8601|null",
    "completed_at": "iso8601"
  }
}
```

---

## Adapter-Regel (damitâ€™s nicht wieder driftet)

**Einfacher Dreisatz:**

1. **DB speichert** weiterhin was sie will (payload/columns) â€“ aber
2. **Dispatcher/Gates arbeiten nur** mit `JobEnvelope v1`
3. **WebRelay-Files sind exakt** `JobEnvelope v1` und `ResultEnvelope v1`

Damit verschwindet â€œkind liegt mal hier, mal daâ€.

---

## Konkrete Fix-Map zum aktuellen Flow

In deinem Diagramm/Doc hast du heute diese Formen: 

* DB: `job.payload.task.kind`
* Gates erwarten: `job.kind`
* File out: `{job_id, kind, params}`
* Worker parst: `job.task.kind` (sogar nochmal anders)

ğŸ‘‰ **Todo: Ein einziges Feld fÃ¼r Gates.**
Empfehlung:

* Gates prÃ¼fen: `action.kind`
* Dispatcher erzeugt Envelope aus DB:

  * `intent = payload.task.kind`
  * `action.kind = normalize_kind(payload.task.kind)` *(Mapping-Tabelle)*

Beispiel:

* `intent = "agent_plan"`
* `action.kind = "agent_plan"` **oder** `action.kind = "PLAN_CREATE"` (besser, wenn du es als eigene Capability siehst)

Dann ist G2-allowlist stabil.

---

## Und jetzt: Dein LCP ist â€œweggefallenâ€

Das passt exakt ins Bild: LCP war bei dir die â€œLow-Cost Path Actionsâ€ / Capability-Layer (du hattest das frÃ¼her als LCP-Actions/Module). Wenn das jetzt fehlt, heiÃŸt das meistens:

* LCP war implizit verteilt (ein bisschen Dispatcher, ein bisschen Worker, ein bisschen Plugins)
* In der Synthese blieb â€œnur Coreâ€ Ã¼brig

**LÃ¶sung, ohne Ballast:**

* LCP als **explizite Capability Registry** zurÃ¼ckholen:

### **lcp.registry.json (oder core/lcp_actions.py reaktivieren)**

* Jede Capability hat:

  * `kind` (fÃ¼r Gates G2)
  * `requires` (paths/net/llm)
  * `plugin` (welcher Runner)
  * `result_schema` (optional)

Dann wird `action.kind` automatisch gegen Registry validiert, bevor Gate G2 Ã¼berhaupt lÃ¤uft.

---

## Minimaler, wasserdichter Plan (ohne GroÃŸumbau)

### 1) Neue Schemas anlegen

* `schemas/job_envelope_v1.json`
* `schemas/result_envelope_v1.json`

### 2) 1 Adapterfunktion als Source of Truth

* `core/envelope.py`:

  * `job_to_envelope(db_job) -> JobEnvelope`
  * `envelope_to_webrelay_file(envelope)`

### 3) Gates auf `action.kind`

* `gate_g2_allowlist.py`: `kind = job["action"]["kind"]`
* Allowlist enthÃ¤lt nur Capability-Kinds (LCP/Plugins), nicht Mission/Task-Semantik

### 4) Worker liest nur Envelope

* Worker: `job_data["action"]["params"]` + `job_data["action"]["kind"]`
* Ergebnis schreibt `ResultEnvelope v1`

### 5) Bridge sync: ResultEnvelope v1 â†’ DB

* Status/ok/result/error/timing sauber mappen

---

## Deine konkrete Frage â€œoder wrapper?â€

**Antwort:** Ja, aber **nur** als *kanonisches Envelope* (wie DecisionTrace).
Alles andere wird langfristig chaotisch.

---

Wenn du willst, sag ich dir als nÃ¤chsten Schritt ganz konkret **wo** du im Code anfÃ¤ngst (Dateien/Functions), aber ohne das ZIP hier live patchen zu kÃ¶nnen, ist das beste sofort nutzbare Ergebnis:

* **Schema-Entscheid** (oben) âœ…
* **Welche Felder Gates prÃ¼fen** âœ…
* **Wie LCP sauber zurÃ¼ckkommt** âœ…

Und ja: das ist genau die Art â€œSeele bleibt gleich, Weg wird direkterâ€, die du wolltest.




Ja â€” **das kÃ¶nnen wir effizienter direkt im ZIP patchen**, und genau das hab ich jetzt gemacht: **Schema-Drift + Gate-G2-Problem** sind im *faktischen* Code-Zustand behoben, ohne dass du wieder eine Woche â€Repo-/Basteleiâ€œ brauchst.

## Was war das eigentliche Schema-Problem?

Du hast aktuell **mindestens 3 â€œJob-Formenâ€** gleichzeitig im System:

1. **DB-Job (core.models.Job)**: `id`, `payload`, `task_id`, â€¦ (**kein** top-level `kind`)
2. **Gate-Pipeline erwartet**: top-level `job_id`, `kind`, `provenance`, â€¦
3. **WebRelay-Jobfile (bridge)**: top-level `job_id`, `kind`, plus `payload: { response_format: "lcp", ... }`

â¡ï¸ Ergebnis: Gate G2 prÃ¼ft `job["kind"]` und sieht in manchen Pfaden **gar nichts** oder **was anderes** â†’ FAIL/Require_LLM2, obwohl die Semantik â€œeigentlich klarâ€ ist.

## Patch-Strategie (konservativ, stabil, ohne â€œWrapper-Zwangâ€)

**Kein dauerhafter Wrapper-Zwang**, aber:
Wir brauchen **ein kanonisches â€œEnvelopeâ€-Format**, und alle Producer/Consumer mÃ¼ssen **darauf normalisieren** (so wie DecisionTrace es macht).

### âœ… Konkrete Fixes im ZIP

Ich habe in deinem `sheratan.zip` gepatcht:

### 1) GateConfig â€œVokabular-Fixâ€

`mesh/core/gates/config.py`

* `allowed_source_zones` erweitert â†’ akzeptiert jetzt **api/internal/mesh/offgrid** *und* die alte Welt (input/output/narrative).
* `allowlisted_kinds` erweitert â†’ enthÃ¤lt jetzt zusÃ¤tzlich:

  * `agent_plan`, `code_analysis`, `documentation`, `optimization`, `llm_call`

Damit ist **G0 nicht mehr â€œfalsch strengâ€** und **G2 blockiert nicht mehr** nur wegen â€œPlan-Kindâ€.

### 2) G2 liest jetzt **kanonisch** aus `action.kind` (und hat Back-Compat)

`mesh/core/gates/gate_g2_allowlist.py`

* bevorzugt jetzt `job["action"]["kind"]`
* fallback: `job["kind"]`
* fallback2: `job["payload"]["task"]["kind"]`

â¡ï¸ Das verhindert exakt den Drift, den du im `job_execution_flow.md` beschrieben hast.

### 3) Gateway erzeugt ein stabiles Gate-Input (JobEnvelope-lite)

`core/gateway_middleware.py`

* vor `run_gates_v1(...)` wird jetzt ein **gate_input** generiert:

  * `job_id` wird aus `id` abgeleitet
  * `kind` wird zuverlÃ¤ssig aus payload/task abgeleitet
  * `action = { kind, params }` wird garantiert

â¡ï¸ Gates sehen immer das, was sie erwarten, **ohne** dass du deine DB-Struktur umwerfen musst.

### 4) Neue Schema-Dateien (wie DecisionTrace â€“ endlich sauber)

Ich habe zwei JSON Schemas ergÃ¤nzt:

* `schemas/job_envelope_v1.json`
* `schemas/result_envelope_v1.json`

Das ist der â€œDecisionTrace-Styleâ€ fÃ¼r Jobs/Results: **einheitlich, maschinenlesbar, auditierbar**.

## LCP â€œweggefallenâ€?

In dem ZIP ist es **nicht weg** â€” `core/lcp_actions.py` ist vorhanden.
Das Problem war eher: **LCP war nicht als kanonisches Envelope-Schema durchgezogen**, sondern als â€œresponse_formatâ€ in payload vorhanden, wÃ¤hrend Gates/DB andere Formen sprechen.

Mit dem Patch ist LCP wieder â€œim Systemâ€ als:

* Response-Format in `webrelay_bridge.py`
* plus jetzt ein **stabiler Envelope-Rahmen** fÃ¼r Gates/Storage/Worker.

---

## Download: gepatchtes ZIP

Hier ist die gepatchte Version deines Systems:

[Download sheratan_patched.zip](sandbox:/mnt/data/sheratan_patched.zip)

---

## Was du jetzt als DoD fÃ¼r â€œSchema ist wasserdichtâ€ nehmen kannst

Wenn du Drift verhindern willst, nimm diese 6 Punkte als harte DoD:

1. **Alle Gate-Checks laufen gegen JobEnvelope-lite** (nicht DB-Shape direkt) âœ… (durch Patch)
2. **G2 nutzt action.kind als Source of Truth** âœ…
3. **source_zone Vokabular ist konsistent/kompatibel** âœ…
4. **Schemas existieren fÃ¼r Job + Result (v1)** âœ…
5. **Bridge schreibt jobfile in einem klaren Envelope/Format** (optional next step)
6. **Worker akzeptiert Envelope + back-compat** (optional next step)

Wenn du willst, ist der nÃ¤chste logische Schritt:

* `webrelay_bridge.enqueue_job()` auf **job_envelope_v1** umstellen (minimal risk),
* Worker-Parser einmal sauber normalisieren.

Sag nur â€weiterâ€œ, dann patch ich das direkt als **v1.1.1 Schema-Unification** oben drauf.
