# prompt_builder.py
"""
Sheratan Prompt Builder
Version: 1.0

Baut den dreischichtigen Prompt für den Sheratan SelfLoop:
- System-Layer (als Referenz)
- Context-Layer (ContextPacket vom Kernel)
- Instruction-Layer (abhängig vom Modus)

Dieses Modul kennt KEINE LLMs, KEINE Tools – nur String-Bau.
"""

from __future__ import annotations
import json
from typing import Any, Dict, Literal

# Self-Loop integration
from .selfloop_prompt_builder import build_selfloop_prompt as build_selfloop_prompt_impl


SYSTEM_REF = "apply SYSTEM_RULESET_SHERATAN_V1\napply INTENT_SUMMARY_V1"


BASE_INSTRUCTION = """You will generate exactly ONE LCP response.

Follow these rules:

1. Produce exactly ONE `decision` object.
2. Produce AT MOST THREE `actions[]`.
3. Only use tools allowed by the mode in `CONTEXT.mission.mode`.
4. If information is insufficient, ASK one clarifying question instead of acting.
5. If the situation appears unsafe or ambiguous, recommend SAFE-MODE.
6. DO NOT repeat SYSTEM or INTENT information.
7. DO NOT include explanations outside the JSON. Use `explanation` inside JSON.
8. Return STRICTLY one JSON object following the LCP specification.
"""


MODE_HINTS = {
    "explore": "Focus on clarifying goals, constraints and high-level strategies. Avoid heavy tool usage.",
    "execute": "Focus on concrete next steps and tool usage. Minimize exploration; prioritize stability and progress.",
    "reflect": "Focus on summarizing results, extracting learnings and proposing improvements. Do not start new missions.",
    "debug": "Focus on diagnosing causes of errors and suggesting safe corrective actions. Avoid risky tools.",
}


LoopMode = Literal["explore", "execute", "reflect", "debug"]


def build_instruction_block(mode: LoopMode = "execute") -> str:
    """
    Baut den Instruction-Layer-Text für das LLM.
    Der Kerntext (BASE_INSTRUCTION) bleibt stabil, pro Modus wird nur ein Fokus-Hinweis ergänzt.
    """
    mode_hint = MODE_HINTS.get(mode, MODE_HINTS["execute"])
    return BASE_INSTRUCTION + f"\nMode hint: {mode_hint}\n"


def serialize_context_packet(context_packet: Dict[str, Any]) -> str:
    """
    Serialisiert den ContextPacket als kompakte JSON- oder YAML-ähnliche Struktur.
    Hier nehmen wir bewusst JSON mit Einrückung, damit das LLM klar erkennt, was Kontext ist.
    """
    # Du kannst das später ersetzen, falls du lieber YAML willst.
    return json.dumps(context_packet, indent=2, ensure_ascii=False)


def build_selfloop_prompt(
    context_packet: Dict[str, Any],
    mode: LoopMode = "execute",
) -> str:
    """
    Baut den vollständigen Prompt für einen SelfLoop-Request an das LLM.

    - SYSTEM-Layer: referenziert die globalen Regeln & den Intent.
    - CONTEXT-Layer: injiziert den ContextPacket.
    - INSTRUCTION-Layer: legt die konkrete Aufgabe in dieser Iteration fest.
    - OUTPUT SPEC: zwingt die LCP-JSON-Struktur.

    Args:
        context_packet: Vom Kernel erzeugter ContextPacket (siehe memory_schema.yaml).
        mode: Aktueller Loop-Modus ("explore", "execute", "reflect", "debug").

    Returns:
        Vollständiger Prompt als String.
    """
    context_str = serialize_context_packet(context_packet)
    instruction_block = build_instruction_block(mode)

    prompt = f"""#####################################################################
# SYSTEM-LAYER (static reference)
#####################################################################
{SYSTEM_REF}


#####################################################################
# CONTEXT-LAYER (dynamically injected by kernel)
#####################################################################
# NOTE: This block is generated by the ContextBuilder.
# It MUST remain compact and structured.
# Raw logs, files, or large texts are NEVER included here.

CONTEXT:
{context_str}


#####################################################################
# INSTRUCTION-LAYER (what you should do NOW)
#####################################################################
{instruction_block}


#####################################################################
# OUTPUT SPEC
#####################################################################
Your entire output MUST be a valid JSON object:

{{
  "decision": {{ ... }},
  "actions": [ ... ],
  "explanation": "optional"
}}

No other text outside the JSON.
"""
    return prompt


def build_prompt_for_job(
    job_payload: Dict[str, Any],
    context_packet: Dict[str, Any],
    mode: LoopMode = "execute",
) -> str | Dict[str, str]:
    """Router function: Choose between LCP and Self-Loop prompt builders.
    
    Args:
        job_payload: The job's payload dict (contains job_type)
        context_packet: Context packet for LCP mode
        mode: Loop mode for LCP
    
    Returns:
        For Self-Loop: Dict with {"system": ..., "user": ...}
        For LCP: String with full prompt
    """
    job_type = job_payload.get("job_type")
    
    if job_type == "sheratan_selfloop":
        # Self-Loop branch
        return build_selfloop_prompt_impl(
            goal=job_payload.get("goal", ""),
            core_data=job_payload.get("core_data", ""),
            current_task=job_payload.get("current_task", ""),
            loop_state=job_payload.get("loop_state", {}),
        )
    else:
        # Default LCP branch
        return build_selfloop_prompt(context_packet, mode)
